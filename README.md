# README Evaluator

[![CI](https://github.com/WillianOliveir1/readme-evaluator/actions/workflows/ci.yml/badge.svg)](https://github.com/WillianOliveir1/readme-evaluator/actions/workflows/ci.yml)

**README Evaluator** is an AI-powered tool that analyses and evaluates GitHub repository README files. It leverages LLMs (Google Gemini or local models via Ollama) to extract structured data based on a comprehensive taxonomy and generates a human-readable quality report with optional PDF export, helping developers improve their project documentation.

## â“ Why README Evaluator?

Documentation is often the first interaction a user has with a project. A poor README can turn away potential users and contributors. This tool provides:

- **Automated Quality Assessment** â€” objective evaluation against a strict JSON schema.
- **Structured Feedback** â€” identifies missing sections (Installation, Usage, License, â€¦).
- **Actionable Improvements** â€” suggests specific changes to enhance clarity and completeness.
- **Real-time Progress** â€” Server-Sent Events stream lets you follow each evaluation step live.
- **PDF Export** â€” download polished PDF reports directly from the UI.
- **Multi-LLM Support** â€” use Google Gemini (cloud) or Ollama (local) with a single env var switch.
- **Dark / Light Theme** â€” sidebar with evaluation history and theme toggle.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js UI  â”‚â”€â”€SSEâ”€â”€â–¶â”‚  FastAPI Backend (Uvicorn)         â”‚
â”‚  :3000       â”‚        â”‚  â”œâ”€ /readme         (download)     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚  â”œâ”€ /extract-json   (evaluate)     â”‚
â”‚  â”‚Dark/Lightâ”‚ â”‚        â”‚  â”œâ”€ /extract-json-stream (SSE)     â”‚
â”‚  â”‚ Theme    â”‚ â”‚        â”‚  â”œâ”€ /render          (report)      â”‚
â”‚  â”‚History   â”‚ â”‚        â”‚  â”œâ”€ /generate        (LLM call)    â”‚
â”‚  â”‚Sidebar   â”‚ â”‚        â”‚  â”œâ”€ /export-pdf      (PDF export)  â”‚
â”‚  â”‚PDF Exportâ”‚ â”‚        â”‚  â”œâ”€ /jobs            (pipeline)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚  â”œâ”€ /cache           (management)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â””â”€ /files           (artifacts)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  LLM Provider         â”‚
                        â”‚  (factory pattern)    â”‚
                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚  Gemini API (default) â”‚
                        â”‚  Ollama (local LLMs)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  MongoDB Atlas        â”‚
                        â”‚  (optional)           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer    | Tech                             |
|----------|----------------------------------|
| Frontend | Next.js 16, React 18, react-markdown, dark/light theme |
| Backend  | Python 3.13, FastAPI, Uvicorn, slowapi (rate limiting) |
| AI       | Google Gemini (`google-genai`) **or** Ollama (local LLMs) |
| PDF      | xhtml2pdf, markdown              |
| Database | MongoDB Atlas (optional â€” falls back to local JSON) |

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+**
- **Node.js 18+** & npm
- **LLM Provider** (one of):
  - **Google Gemini API Key** â€” get one at [Google AI Studio](https://aistudio.google.com/) *(default)*
  - **Ollama** â€” install from [ollama.ai](https://ollama.ai/) for local LLM inference

### 1 â€” Clone & configure

```bash
git clone https://github.com/WillianOliveir1/readme-evaluator.git
cd readme-evaluator
cp .env.example .env          # edit .env and set GEMINI_API_KEY
```

### 2 â€” Backend

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate

pip install --upgrade pip
pip install -r backend/requirements.txt
```

### 3 â€” Frontend

```bash
cd frontend
npm install
cd ..
```

### 4 â€” Run

```bash
# Terminal 1 â€” backend
python -m uvicorn backend.main:app --reload --host 127.0.0.1 --port 8000

# Terminal 2 â€” frontend
cd frontend && npm run dev
```

Open **http://localhost:3000**, paste a GitHub repo URL, and click **Evaluate README**.

## ğŸ³ Docker

The quickest way to run the whole stack:

```bash
cp .env.example .env           # set at least GEMINI_API_KEY
docker compose up --build
```

| Service  | URL                     |
|----------|-------------------------|
| Backend  | http://localhost:8000   |
| Frontend | http://localhost:3000   |

See [docker-compose.yml](docker-compose.yml) for all configurable environment variables.

## âš™ï¸ Configuration

All configuration is done via environment variables (or the `.env` file).  
Copy `.env.example` for a documented list of every option:

| Variable            | Required | Default              | Description |
|---------------------|----------|----------------------|-------------|
| `GEMINI_API_KEY`    | **Yes**Â¹ | â€”                    | Google Gemini API key |
| `LLM_PROVIDER`      | No       | `gemini`             | LLM backend: `gemini` or `ollama` |
| `OLLAMA_BASE_URL`   | No       | `http://localhost:11434` | Ollama API URL (when `LLM_PROVIDER=ollama`) |
| `OLLAMA_MODEL`      | No       | `llama3`             | Default Ollama model name |
| `GITHUB_TOKEN`      | No       | â€”                    | Raises GitHub rate limit to 5 000 req/h |
| `API_KEY`           | No       | â€”                    | When set, every request must include `X-API-Key` header |
| `MONGODB_URI`       | No       | â€”                    | MongoDB connection string; unset = local JSON storage |
| `MONGODB_DB`        | No       | `readme_evaluator`   | MongoDB database name |
| `LOG_LEVEL`         | No       | `INFO`               | `DEBUG`, `INFO`, `WARNING`, `ERROR` |
| `LOG_FORMAT`        | No       | `json`               | `json` or `text` |
| `CORS_ORIGINS`      | No       | `localhost:3000`     | Comma-separated allowed origins |
| `DEFAULT_RATE_LIMIT`| No       | `60/minute`          | Global rate limit per client IP |
| `EXPENSIVE_RATE_LIMIT`| No     | `10/minute`          | Rate limit for AI/PDF endpoints |
| `MAX_CONCURRENT_PIPELINES` | No | `3`                | Max pipeline jobs running in parallel |

> Â¹ Required only when `LLM_PROVIDER=gemini` (default).

## ğŸ§ª Testing

```bash
# Run the full test suite (314 tests)
python -m pytest tests/ -q

# Type-checking
python -m mypy backend/ --ignore-missing-imports
```

The CI pipeline (GitHub Actions) runs both on every push and pull request to `main`.

## ğŸ“‚ Project Structure

```
readme-evaluator/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py               # FastAPI entrypoint + middleware
â”‚   â”œâ”€â”€ config.py              # Centralised settings
â”‚   â”œâ”€â”€ logging_config.py      # Structured JSON / text logging
â”‚   â”œâ”€â”€ models.py              # Pydantic request/response models
â”‚   â”œâ”€â”€ pipeline.py            # Multi-step evaluation pipeline (semaphore + locks)
â”‚   â”œâ”€â”€ llm_base.py            # Abstract LLMClient base class
â”‚   â”œâ”€â”€ gemini_client.py       # Gemini LLM client (with retry)
â”‚   â”œâ”€â”€ ollama_client.py       # Ollama local LLM client (with retry)
â”‚   â”œâ”€â”€ llm_factory.py         # Factory: get_llm_client() by LLM_PROVIDER
â”‚   â”œâ”€â”€ rate_limit.py          # slowapi rate limiting config
â”‚   â”œâ”€â”€ cache_manager.py       # Temp file lifecycle
â”‚   â”œâ”€â”€ prompt_builder.py      # Prompt construction
â”‚   â”œâ”€â”€ routers/               # 8 API routers
â”‚   â”‚   â”œâ”€â”€ readme.py          # POST /readme
â”‚   â”‚   â”œâ”€â”€ extract.py         # POST /extract-json, /extract-json-stream
â”‚   â”‚   â”œâ”€â”€ render.py          # POST /render, /render-evaluation
â”‚   â”‚   â”œâ”€â”€ generate.py        # POST /generate
â”‚   â”‚   â”œâ”€â”€ export_pdf.py      # POST /export-pdf
â”‚   â”‚   â”œâ”€â”€ jobs.py            # GET /jobs, POST /jobs, GET /jobs/{id}
â”‚   â”‚   â”œâ”€â”€ cache.py           # GET /cache/stats, POST /cache/cleanup
â”‚   â”‚   â””â”€â”€ files.py           # GET /files/{path}
â”‚   â”œâ”€â”€ download/              # GitHub README downloader
â”‚   â”œâ”€â”€ evaluate/              # JSON extraction & validation
â”‚   â”œâ”€â”€ present/               # Report renderer
â”‚   â”œâ”€â”€ db/                    # MongoDB persistence layer
â”‚   â””â”€â”€ prompts/               # System prompt templates
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ _app.js            # Custom App (global CSS)
â”‚   â”‚   â””â”€â”€ index.js           # Main page (Sidebar, Progress, Report, PDF)
â”‚   â””â”€â”€ styles/
â”‚       â””â”€â”€ globals.css        # Global styles (dark/light theme, 654 lines)
â”œâ”€â”€ schemas/                   # JSON Schema taxonomy
â”œâ”€â”€ tests/                     # pytest suite (314 tests)
â”œâ”€â”€ tools/                     # CLI utilities & analysis scripts
â”œâ”€â”€ Dockerfile.backend
â”œâ”€â”€ Dockerfile.frontend
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml             # mypy configuration
â””â”€â”€ .github/workflows/ci.yml   # GitHub Actions CI
```

## ğŸ“… Status & Roadmap

**Current Status:** Active Development (Beta)

- [x] Core extraction pipeline with Gemini
- [x] Streaming response (SSE) for real-time feedback
- [x] MongoDB integration for persistence
- [x] REST API with authentication (X-API-Key)
- [x] Structured logging (JSON / text)
- [x] Type-checked codebase (mypy strict, 0 errors)
- [x] 314 unit & integration tests with pytest
- [x] Docker support (docker compose)
- [x] CI/CD with GitHub Actions
- [x] Rate limiting (slowapi) & concurrency control
- [x] Retry with exponential backoff (tenacity)
- [x] Ollama / local LLM support (factory pattern)
- [x] PDF export of evaluation reports
- [x] Frontend redesign (dark/light theme, sidebar history, progress bar)
- [ ] Batch processing for multiple repositories
- [ ] Comparative analysis between multiple READMEs

## ğŸ‘¥ Authors

- **Willian Oliveira** â€” *Initial work* â€” [WillianOliveir1](https://github.com/WillianOliveir1)

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository.
2. Create a feature branch (`git checkout -b feature/AmazingFeature`).
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4. Push to the branch (`git push origin feature/AmazingFeature`).
5. Open a Pull Request.

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see [LICENSE](LICENSE) for details.

## ğŸ“š References

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Next.js Documentation](https://nextjs.org/docs)
- [Google AI Studio](https://aistudio.google.com/)
- [Google GenAI Python SDK](https://github.com/googleapis/python-genai)

